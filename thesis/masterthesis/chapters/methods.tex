\chapter{Methods}
\section{vSLAM Algorithms}

	\subsection{Definitions}
	
		\subsubsection{Covisibility Graph}
		
		updating the edges resulting from the shared map
		points with other keyframes.
		
		\subsubsection{Keyframe}
		
		Most SLAM Algorithms make usage of so called keyframes, as these keyframe-based approaches have proven to be more accurate \cite{keyframe}. 
		
		\subsubsection{Direct and Feature based methods}
		
		\subsubsection{Group of Rigid Transformations in 3D}
		
		$\text{SE}(3)$ is the group of rigid transformations in 3D space \cite{se3}. Each Matrix $ T \in \mathbb{R}^{4x4}$ with 
		
		$$ T = \left[
		\begin{array}{rrr}
		R &  t \\  
		0  & 1 \\ 
		\end{array} \right]$$
		
		and $ R \in \mathbb{R}^{3x3}$ beiing a rotation matrix and $ t \in \mathbb{R}^{3}$ a translational vector, is an element of $\text{SE}(3)$.
		

	\subsection{ORB-SLAM}
	
	ORB SLAM is a feature based, state of the art slam method. The first version was published in 2015 \cite{orb}. 
	Here, an overview of the functionality of ORB SLAM is provided. The Algorithms runs on three threats simultanously.
	Each thread performes one of the following tasks: Tracking, Local Mapping and Loop Closing. An overview over the tasks can be found 
	in figure \ref{fig:orb_fig}. The explaination of these system components are described in the following subsections. 
	A more detailed explaination can be found in the paper of Raul Mur-Artal et al \cite{orb}.
	
	\fig{img/orb_slam_overview.png}{Overview of the system components extracted from \cite{orb}}{fig:orb_fig}{0.6}
	
	\subsubsection{Tracking}
	
	The tracking component determines the localization of the camera and decides, when a new keyframe is beeing inserted.
	As it is shown in figure \ref{fig:orb_fig}, the tracking is performed in four steps.
	
	\begin{enumerate}
	\item Feature Extracting 
	
	Features are extracted using Oriented FAST and Rotated BRIEF \cite{orb_feat}. This method starts by searching for 
	FAST (Features from Accelerated and Segments Test). Herefor, for each pixel x in the image, a circle of 16 pixels around that pixels
	is considered and checked if at least eight of these 16 pixels have major brightness differences. If so, the pixel x is considered as 
	a keypoint, since it is likely to be an edge or corner. This is repeated again and again after downsizing the image up to a scale of eight. 
	To extract features evenly distributed over the image, it is devided into a grid, trying to extract five features per cell. 
	Extracting features this way, makes the algorithm more stable to scale invariance. 
	Next the orientation of the extracted feature is calculated using a intensity centroid. 
	Finally the features are converted into a binary vectors (ORB descriptor) using a modified version, which is more robust to rotation, of BRIEF descriptors (Binary robust independent elementary feature).
	% wenn ich noch mehr brauch kann ich hier noch was adden. These ORB descriptors are then used for all feature matching tasks. 
	
	
	
	\item Initial Pose Estimation
	
	A constant velocity model is first run to predict to the camera pose. Then, the features of the last frame are searched. If no matches are found, 
	a wider area around the last position is searched. 
	
	\item Track Local Map 
	
	When the camera pose is estimated, map point correspondences are searched in 
	the local map, containing keyframes that contain the observed map points and
	the keyframes from the covisibility graph. The pose is then corrected with all
	matched mappoints. 
	
	% wenn ich noch mehr brauche, hier weitermachen.
	
	
	\item New Keyframe Decision 
	
	To insert the current frame as a keyframe, the following conditions have to be met:
	more than 20 frames have to be passed from the last relocalization or keyframe insertion (when not idle), 
    the current frame tracks at least 50 points or less than 90 percent of the points of the keyframe in the local 
    map with the most shard mappoints. 	
	
	
	\end{enumerate}
	

	
	\subsubsection{Local Mapping}
	
	Whenever a new Keyframe $K_i$ is inserted, the map is updated. 
	
	
	\begin{enumerate}
	\item{KeyFrame Insertion}
	
	The keyframe is inserted in the covisibility graph. Then the spanning tree
	is updated using using the keyframe with the most common points with $K_i$.
	Finally the keyframe is represented as a bag of words using the DBoW2 implementation. 
	Therefor, the image is saved by the number of occurances of features found in a a predefined
	vocabulary of features. When the vocabulary is created with images general enough, 
	it can be used for most environments.
	
	% cite

	
	\item{Recent Map Points Culling}
	
	A mappoint is removed from the map, when it is found in more than 
	25/% in the frames where it is predicted to be visible. Also, 
	it must be observed from more than two keyframes if more than one keyframe 
	has passed from map point creation. 
	
	\item{New Map Point Creation}
	
	A map point is created by calculating the triangulation of the connected
	keyframes in the covisibility graph. For each map point, the 3D coordinate 
	in the world coordinate system, its ORB descriptor, the viewing direction, 
	the maximim and minimum distance at which the point can be observed is stored. 
	
	\item{Local Bundle Adjustment}
	
	The keyframe poses $T_i \in \text{SE}(3)$ and Map Points $X_j \in \mathbb{R}^{3}$ 
	are optimized by minizing the reprojection error to the matched keypoints $x_{i,j} \in \mathbb{R}^{2}$.
	The error is computed by the following term:
	
	$$ e_{i,j} = x_{i,j} - \pi_i\left(T_i, X_j\right) $$. 
	$i$ is the respective Keyframe and j the index of the map Point. 
	$\pi_i$ is a projection function, calculation a transformation 
	to project all keypoints on mappoints by minimizing a cost function, that
	can be found in \cite{ba}.
	% citat anfang
	In case of full BA
	(used in the map initialization) we optimize
	all points and keyframes, by the exception of the first
	keyframe which remain fixed as the origin. In local BA
	all points included in the local area
	are optimized, while a subset of keyframes is fixed. In
	pose optimization, or motion-only BA, all
	points are fixed and only the camera pose is optimized.
	% citat ende
	At this point, a local BA is performed.
	
	\item{Local Keyframe Culling}
	
	With difference to other SLAM algorithm, ORB slam delets redundant 
	keyframes, which decreases computational efforts, since computational
	complexity grows with the number of keyframes. All keyframes are deleted, 
	where at least 90 percest of the map points can be found in at least three other 
	keyframes. 
	
	
	\end{enumerate}
	
	\subsubsection{Loop Closing}
	
	The loop closing is computed based on the last inserted keyframe $K_i$. 
	
	\begin{enumerate}
	\item{Loop Candidates Detection}
	
	First the similarity of $K_i$ to its neighbours in the covisibilty
	graph is computed by using the bag of words representation and a 
	loop candidate $K_l$ might be chosen. 
	
	\item{Similarity Transformation}
	
	In this step the transformation is computet, to map the map points
	from $K_i$ on $K_l$. Since scale can drift, also the scale is computet
	in addition to the rotation matrix and translation using the mothod of horn. 
	
	
	\item{Loop Fusion}
	
	Here, duplicated map points are fused and the keyframe pose $T_\omega$ is corrected by the transformation
	calculated in the previous step. All map points of $K_l$ are projected in $K_i$. 
	All keyframes affected by the fusion will update the edjes (shared map points) in the 
	covisibilty graph. 
	
	\item{ Essential Graph Optimization}
	
	Finally the loop closing error is distributed over the essential graph. 
	
	% essential graph hier noch beschreiben. 
	
	\end{enumerate}
	
	
	
	
	
	

	

	\subsection{DSM-SLAM}

	\subsection{DSO-SLAM}
	

\section{Calculations} \label{calc}

	\subsection{Trajectory Alignment}
	
	In order to compare the evaluated position of the camera at a given time with the ground truth of the 
	position, the trajectories need to be aligned. This is because most SLAM Algorithms innitilize the origin
	of their coordinate system with the camera position from the first frame. Whereas the ground truth of the 
	trajectory uses a different origin. As a consequence, evaluated points $ \left\{{\widehat{p}_i}\right\}_{i=0}^{N-1} $ can not be 
	compared to the ground truth points $ \left\{{p_i}\right\}_{i=0}^{N-1} $
	Also, as described in the vSLAM Algorithms section,
	
	% input link of section here
	
	the minority of the existing vSLAM algorithms are recognizing the true scale of the coordinate system. For
	those two reasons, the target is to find $S = \left\{R,t,s\right\}$, while $R$ being a rotation matrix, $t$ a translation vector
	and $s$ a scaling factor, 
	such that
	
	$$ S = \argmin_{S' = \left\{R', t', s' \right\}} \sum_{i = 0}^{N-1} \norm{p_i - s'R'\widehat{p}_i - t'}^2 $$ .
	
	In other words, the evaluated points are rotated, translated and scaled in a way, that the sum squared error over the point
	distances is minimized. The upper expresssion is calculated by using the method of Umeyama \cite{ume}. 
	
	Similar to principal component analysis, Umeyama uses the singular value decomposition of the covarianve 
	matrix $\Sigma$ of $p$ and $\widehat{p}$. Thus, 
	$\Sigma = UDV^T$ is yielded. Umeyama proves, that $R,t$ and $s$ can be calculated as followed: 
	
	$$ R = UWV^T $$
	$$ s = \frac{1}{\sigma^2_p}\text{tr}\left(DW\right)$$
	$$ t = \mu_{\widehat{p}} - sR\mu_p $$
	
	with 
	
	$$ W = \begin{cases}
      I, & \text{if}\ \text{det}\left(U\right)\text{det}\left(V\right) =0 \\
      \text{diag}\left(1,1,-1\right), & \text{otherwise}
    \end{cases}$$
	$\sigma_p$ beeing the standard deviation of $p$, $\mu$ the mean and $\text{tr}$ the trace of a matrix. 
	

\section{Datasets}

	\subsection{EuRoC Dataset}

	For the evaluation of the vSLAM Algorithms, the EuRoC dataset \cite{euroc} was used.
	The dataset contains eleven video sequences, recorded with a micro aerial vehicle at 20 frames per second.
	The sequences have a resultion of 752x480 pixels.
	For each Sequence, RGB images from two cameras exist. However, since the evaluation
	focuses on monocular SLAM methods, only the left camera was considered. Also the available 
	inertial and camera pose data was not taken in consideration. The first five sequences were recorded in 
	the machine hall at ETH Zürich, and the other six were recorded in a room, that was provided 
	with additional obsticals. For the latter six sequences, the groundtruth of the environment 
	exists as a dense pointcloud, as can be seen in figure \ref{fig:pcgt}.

	\fig{img/pointcloud_gt.png}{Pointcloud ground truth of sequence V1\_01\_easy visualized with python package pptk}{fig:pcgt}{0.7}

	Finally the true position of the 
	camera is known at a high frequency of over 200 points per second. 
	An overview of the sequences is shown in table \ref{table:euroctable}.



	\begin{table}
	\caption{Overview of the sequences included in the EuRoC Dataset}
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|}
	\hline
	Sequence Name& Duration in $s$ & Average Velocity in $ms^{-1}$ &Pointcloud available\\
	\hline
	MH\_01\_easy & 182 & 0.44 & No\\
	MH\_02\_easy & 150 & 0.49 & No\\
	MH\_03\_medium & 132 & 0.99 & No\\
	MH\_04\_difficult & 99 & 0.93 & No\\
	MH\_05\_difficult & 111 & 0.88 & No\\
	V1\_01\_easy & 144 & 0.41 & Yes\\
	V1\_02\_medium & 83.5 & 0.91 & Yes\\
	V1\_03\_difficult & 105 & 0.75 & Yes\\
	V2\_01\_easy & 112 & 0.33 & Yes\\
	V2\_02\_medium & 115 & 0.72 & Yes\\
	V2\_03\_difficult & 115 & 0.75 & Yes\\
	\hline
	\end{tabular}
	\label{table:euroctable}
	\end{table}
	
\section{Setup and Environment}

	\subsection{Evaluation}
	
	The entire evaluation is run on a virtual machine. The host system is a lenovo yoga with eight GB of RAM and the basic model (8250U CPU @1.6 
	GHz 1.80GHz) of an eight core i5. The operating system of the host machine is Windows 10 Home. The virtual
	machine is given 5 GB of Ram and 4 cores. The operating system of the virtual machine is Ubuntu 18.04. All further setup information can be extracted 
	from the github repository.

	\subsection{Flight Path Planning}
 
 