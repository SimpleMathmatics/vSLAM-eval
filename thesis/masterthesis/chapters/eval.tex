\section{Evaluation Methods}\label{methsec}

To evaluate the performance of DSO-, DSM- and ORB-SLAM, the algorithms are fed with video sequences contained in the 
European Robotics Challenge (EuRoC) benchmark dataset. The algorithms are changed in a way, that they save the computed 
trajectory estimation to a .txt file and the resulting point cloud to a .PLY file. 

Because an algorithm run on the same sequence can yield different results, each sequence is run three times for each algorithm. 
This way, the results are more reproducible. 

\subsection{Dataset}

	For the evaluation of the vSLAM Algorithms, the EuRoC dataset \cite{euroc} was used.
	The dataset contains eleven video sequences, recorded with a micro aerial vehicle at 20 frames per second.
	The sequences have an image resolution of 752x480 pixels.
	
	For each sequence, RGB images from two cameras exist. However, since the evaluation
	focuses on monocular SLAM methods, only the left camera was considered. Also the available 
	inertial and camera pose data was not taken in consideration. 
	
	The first five sequences were recorded in 
	the machine hall of the Eidgenössische Technische Hochschule Zürich, and the other six were recorded in a room, that was provided 
	with additional obstacles. For the latter six sequences, the ground truth of the environment 
	exists as a dense point cloud, as can be seen in figure \ref{fig:pcgt}.

	\fig{img/pointcloud_gt.png}{Point cloud ground truth of sequence V1\_01\_easy visualized with python package pptk}{fig:pcgt}{0.9}

	Finally the true camera pose $\in \text{SE}(3)$ of the 
	camera is known at a high frequency of over 200 points per second. This includes the camera position and the camera orientation. 
	
	An overview of the sequences is shown in table \ref{table:euroctable}. For each sequence, the average camera velocity,
	rotation velocity and duration 
	are provided. 



	\begin{table}
	\caption{Overview of the sequences included in the EuRoC Dataset}
	\begin{tabular}{ |p{3cm}||p{3cm}|p{2cm}|p{2cm}|p{3cm}|}
	\hline
	Sequence Name& Duration in $s$ & Average Velocity in $ms^{-1}$ & Angular Velocity in $rad s^{-1}$ &Point cloud available\\
	\hline
	MH\_01\_easy & 182 & 0.44 & 0.22 & No\\
	MH\_02\_easy & 150 & 0.49 & 0.21 &No\\
	MH\_03\_medium & 132 & 0.99 & 0.29 & No\\
	MH\_04\_difficult & 99 & 0.93 & 0.24 & No\\
	MH\_05\_difficult & 111 & 0.88 & 0.21 & No\\
	V1\_01\_easy & 144 & 0.41 & 0.28 & Yes\\
	V1\_02\_medium & 83.5 & 0.91 & 0.56 & Yes\\
	V1\_03\_difficult & 105 & 0.75 & 0.62 & Yes\\
	V2\_01\_easy & 112 & 0.33 & 0.28 & Yes\\
	V2\_02\_medium & 115 & 0.72 & 0.59 & Yes\\
	V2\_03\_difficult & 115 & 0.75 & 0.66 & Yes\\
	\hline
	\end{tabular}
	\label{table:euroctable}
	\end{table}

\subsection{Evaluation Criteria}
\subsubsection{Trajectory Comparison}

	\begin{enumerate}
	\item{Trajectory Alignment}\label{umeyamesec}
	
	In order to compare the evaluated position of the camera at a given time with the ground truth of the 
	position, the trajectories need to be aligned. This is because most SLAM Algorithms initialize the origin
	of their coordinate system with the camera position from the first frame. Whereas the ground truth of the 
	trajectory uses a different origin and orientation. As a consequence, evaluated points $ \left\{{\widehat{x}_i}\right\}_{i=0}^{N-1}$ can not be 
	compared to the ground truth points $\left\{{x_i}\right\}_{i=0}^{N-1}$
	Also, as described in the vSLAM Algorithms section,
	
	% input link of section here
	
	the minority of the existing vSLAM algorithms are recognizing the true scale of the coordinate system. For
	those two reasons, the target is to find $S = \left\{R,t,s\right\}$, while $R$ being a rotation matrix, $t$ a translation vector
	and $s$ a scaling factor, 
	such that
	
	\begin{equation}
	S = \argmin_{S' = \left\{R', t', s' \right\}} \sum_{i = 0}^{N-1} \norm{x_i - s'R'\widehat{x}_i - t'}^2
	\end{equation}\label{alignmin}
	
	In other words, the evaluated points are rotated, translated and scaled in a way, that the sum squared error over the respective point
	distances is minimized. The upper expression is calculated by using the method of Umeyama \cite{ume}. 
	
	Similar to a principal component analysis, Umeyama uses the singular value decomposition of the covariance 
	matrix $\Sigma$ of $x$ and $\widehat{x}$. Thus, 
	$\Sigma = UDV^T$ is yielded. Umeyama proves, that $R,t$ and $s$ can be calculated as followed: 
	
	$$ R = UWV^T $$
	$$ s = \frac{1}{\sigma^2_p}\text{tr}\left(DW\right)$$
	$$ t = \mu_{\widehat{x}} - sR\mu_p $$
	
	with 
	
	$$ W = \begin{cases}
      I, & \text{if}\ \text{det}\left(U\right)\text{det}\left(V\right) =0 \\
      \text{diag}\left(1,1,-1\right), & \text{otherwise}
    \end{cases}$$
	$\sigma_p$ being the standard deviation of $x$, $\mu$ the mean and $\text{tr}$ the trace of a matrix. 
	
	\item{Positional Error}
	
	The error between $\left\{{\widehat{x}_i}\right\}_{i=0}^{N-1}$ and $\left\{{x_i}\right\}_{i=0}^{N-1}$ is computed after aligning them 
	with the upper method using the computed parameters $S = \left\{R,t,s\right\}$, yielding
	
	$$ \widehat{x}_i' = sR\widehat{x}_i - t $$
	
	. Then the distances between the points are evaluated using the euclidean norm: 
	
	$$ e_i = \lVert \mathbf{\widehat{x}_i' - x_i} \rVert_2$$
	
	\fig{img/trajectory_error.png}{Trajectory error after alignment. Source: \cite{tutorial}}{fig:trerror}{0.8}
	
	In figure \ref{fig:trerror} the computed errors are displayed. 
	
	
	These error terms are visualized over time and the overall mean is determined and again visualized with boxplots for each method.
	Additionally, the flight paths are plotted against each other after alignment, to gain visual information of the trajectories. 
	This is done for all three axes. 

	
	\end{enumerate}
	
\subsubsection{Point cloud evaluation}

The algorithms were manipulated in a way, that after evaluating each sequence, they write a .PLY file with all map points to 
the device. These map points are then evaluated by the following methods. Obviously, this is only done for the latter six sequences, 
where a ground truth of the point cloud exists. 

Additionally to the following methods, the point clouds are visually observed, 
trying to figure out, if the SLAM algorithms are also able to detect small obstacles, which is crucial for a successful autonomous
 navigation of the drone. 
	
	\begin{enumerate}
	\item{Positional Error}
	
	Again, the map points are transformed using the method of Umeyama. However, it is crucial to note that the computed 
	$S = \left\{R,t,s\right\}$ is not a result of aligning the point clouds, but rather the parameters for aligning the trajectories 
	are used. This is done, to ensure, that trajectory and point cloud are transformed in the same way, and fit in the same reference frame. 
	
	To compare the the transformed computed point cloud $P' = \left\{{\widehat{p'}_i}\right\}_{i=0}^{M_{\text{eval}}-1}$ to the ground truth point cloud
	$\left\{p_i\right\}_{i=0}^{M_{\text{gt}}-1}$, for a point in the evaluated point cloud, the distance to the closest point in the ground truth
	point cloud is calculated. This is assumed to be the points ground truth position, since with several 100000-points in ground truth, this point in 
	the ground truth point cloud should not be too far away from the orthogonal projection of the evaluated point. 
	
	\fig{img/orthProj.png}{Orthogonal projection of a point in the evaluated point cloud ($\widehat{p_1}$) on the planar of the ground truth point cloud.
	 The error $e$, determines how far the considered point for the ground truth lies from the actual point of the ground truth.}{fig:orth}{0.7}
	
	This situation is displayed in figure \ref{fig:orth}, where it gets clear, that the more points are available in the ground truth point cloud, 
	the smaller is the distance in between them and therefore the smaller the error $e$ becomes.

	Since calculating distances from several 100000 points to several 100000 points is computational very expensive, and in the current setup applying 
	it an all sequences and algorithms would require more than a day, only a subset of $P'$ of 1000 points per sequence and algorithms
	is taken into consideration. The indices for the subset $I_{\text{sub}}$ are sampled from a even distribution of ${i}_{i = 0}^{M_{\text{eval}}-1}$. 
	Then, as mentioned, distances to the closest point in the ground truth point cloud is calculated for the sampled subset 
	$P'_{\text{sub}} = \left\{{\widehat{p'}_i}\right\}_{i \in I_{\text{sub}}} \subset P'$. 
	
	The error term for $\widehat{p}_i' \in P'_{\text{sub}}$ is then given by
	$$ e_i = \argmin_{j \in {i}_{i=0}^{M_{\text{gt}}-1}} \lVert \mathbf{\widehat{p}_i' - p_j} \rVert_2 $$.
	
	These error terms are then plotted within a boxplot for each method over all sequences. 
	
	\item{Density}
	
	As described in the second chapters, as a result of the functionality behind feature based methods, their evaluated point clouds are 
	significantly less dense. To quantify the density, for each algorithm and sequence the absolute number of points generated by the 
	algorithm is accessed. 
	
	\end{enumerate}
	
\subsubsection{Computation Time}
	
	Since the computational performance of an algorithm is crucial to perform in real time, the absolute time that is needed to process each 
	sequence is measured for each algorithm. The time required for initialization is subtracted, since it is not decisive for the assessment, 
	if the algorithm can be run in real time. For each sequence the resulting speed is additionally evaluated in computed frames per second. 
	
\subsection{Setup and Environment}

	
	The entire evaluation is run on a virtual machine. The virtual machine is running through the program VirtualBox provided by oracle.
	The host system is a Lenovo yoga notebook with eight GB of RAM and the basic model (8250U CPU @1.6 
	GHz 1.80GHz) of an eight core i5. The operating system of the host machine is Windows 10 Home. The virtual
	machine is given 5 GB of Ram and 4 cores for the computations. The operating system of the virtual machine is Ubuntu 18.04. All further setup information can be extracted 
	from the github repository.
