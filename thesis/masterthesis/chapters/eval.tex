\chapter{Evaluation Methods}

\section{Datasets}

	\subsection{EuRoC Dataset}

	For the evaluation of the vSLAM Algorithms, the EuRoC dataset \cite{euroc} was used.
	The dataset contains eleven video sequences, recorded with a micro aerial vehicle at 20 frames per second.
	The sequences have a resultion of 752x480 pixels.
	
	For each Sequence, RGB images from two cameras exist. However, since the evaluation
	focuses on monocular SLAM methods, only the left camera was considered. Also the available 
	inertial and camera pose data was not taken in consideration. The first five sequences were recorded in 
	the machine hall at ETH ZÃ¼rich, and the other six were recorded in a room, that was provided 
	with additional obsticals. For the latter six sequences, the groundtruth of the environment 
	exists as a dense pointcloud, as can be seen in figure \ref{fig:pcgt}.

	\fig{img/pointcloud_gt.png}{Pointcloud ground truth of sequence V1\_01\_easy visualized with python package pptk}{fig:pcgt}{0.7}

	Finally the true position of the 
	camera is known at a high frequency of over 200 points per second. 
	An overview of the sequences is shown in table \ref{table:euroctable}.



	\begin{table}
	\caption{Overview of the sequences included in the EuRoC Dataset}
	\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|p{3cm}|}
	\hline
	Sequence Name& Duration in $s$ & Average Velocity in $ms^{-1}$ &Pointcloud available\\
	\hline
	MH\_01\_easy & 182 & 0.44 & No\\
	MH\_02\_easy & 150 & 0.49 & No\\
	MH\_03\_medium & 132 & 0.99 & No\\
	MH\_04\_difficult & 99 & 0.93 & No\\
	MH\_05\_difficult & 111 & 0.88 & No\\
	V1\_01\_easy & 144 & 0.41 & Yes\\
	V1\_02\_medium & 83.5 & 0.91 & Yes\\
	V1\_03\_difficult & 105 & 0.75 & Yes\\
	V2\_01\_easy & 112 & 0.33 & Yes\\
	V2\_02\_medium & 115 & 0.72 & Yes\\
	V2\_03\_difficult & 115 & 0.75 & Yes\\
	\hline
	\end{tabular}
	\label{table:euroctable}
	\end{table}

\section{Trajectory Comparison}

	\subsection{Trajectory Alignment}
	
	In order to compare the evaluated position of the camera at a given time with the ground truth of the 
	position, the trajectories need to be aligned. This is because most SLAM Algorithms innitilize the origin
	of their coordinate system with the camera position from the first frame. Whereas the ground truth of the 
	trajectory uses a different origin. As a consequence, evaluated points $ \left\{{\widehat{x}_i}\right\}_{i=0}^{N-1}$ can not be 
	compared to the ground truth points $\left\{{x_i}\right\}_{i=0}^{N-1}$
	Also, as described in the vSLAM Algorithms section,
	
	% input link of section here
	
	the minority of the existing vSLAM algorithms are recognizing the true scale of the coordinate system. For
	those two reasons, the target is to find $S = \left\{R,t,s\right\}$, while $R$ being a rotation matrix, $t$ a translation vector
	and $s$ a scaling factor, 
	such that
	
	$$ S = \argmin_{S' = \left\{R', t', s' \right\}} \sum_{i = 0}^{N-1} \norm{x_i - s'R'\widehat{x}_i - t'}^2 $$ .
	
	In other words, the evaluated points are rotated, translated and scaled in a way, that the sum squared error over the point
	distances is minimized. The upper expresssion is calculated by using the method of Umeyama \cite{ume}. 
	
	Similar to principal component analysis, Umeyama uses the singular value decomposition of the covarianve 
	matrix $\Sigma$ of $x$ and $\widehat{x}$. Thus, 
	$\Sigma = UDV^T$ is yielded. Umeyama proves, that $R,t$ and $s$ can be calculated as followed: 
	
	$$ R = UWV^T $$
	$$ s = \frac{1}{\sigma^2_p}\text{tr}\left(DW\right)$$
	$$ t = \mu_{\widehat{x}} - sR\mu_p $$
	
	with 
	
	$$ W = \begin{cases}
      I, & \text{if}\ \text{det}\left(U\right)\text{det}\left(V\right) =0 \\
      \text{diag}\left(1,1,-1\right), & \text{otherwise}
    \end{cases}$$
	$\sigma_p$ beeing the standard deviation of $x$, $\mu$ the mean and $\text{tr}$ the trace of a matrix. 
	
	\subsection{Positional Error}
	
	The error between $\left\{{\widehat{x}_i}\right\}_{i=0}^{N-1}$ and $\left\{{x_i}\right\}_{i=0}^{N-1}$ is computet after aligning them 
	with the upper method using the computet parameters $S = \left\{R,t,s\right\}$, yielding
	
	$$ \widehat{x}_i' = sR\widehat{x}_i - t $$
	
	. Then the distances between the points are evaluated using the eucledean norm: 
	
	$$ e_i = \lVert \mathbf{\widehat{x}_i' - x_i} \rVert_2$$
	
	\fig{img/trajectory_error.png}{Trajectory error after alignment. Source: \cite{tutorial}}{fig:trerror}{0.5}
	
	In figure \ref{fig:trerror} the computet errors are displayed. 
	
	
	These error terms are visualized over time and the overall mean is determined and again visualized with boxplots for each method.
	Additionally, the flight paths are plotted against each other after alignment, to gain visual information of the trajectories. 
	This is done for all three axes. 
	
\section{Pointcloud evaluation}

The algorithms were manipulated in a way, that after evaluating each sequence, they write a .PLY file with all map points to 
the device. These map points are then evaluated by the following methods. Obviously, this is only done for  latter six sequences, 
where a groundtruth of the pointcloud exists. Additionally to the following methods, the point clouds are visually observed, 
trying to figure out, if the SLAM algorithms are also able to detect small obstacles, which is crucial for a successfull navigation. 

	\subsection{Positional Error}
	
	Again, the map points are transformed using the method of umeyama. However, it is crucial to note that the computed 
	$S = \left\{R,t,s\right\}$ is not a result of aligning the point clouds, but rather the parameters for aligning the trajectories 
	are used. This is done, to ensure, that trajectory and point cloud are transformed in the same way, and fit in the same world reference. 
	
	To compare the the transformed computet point cloud $P' = \left\{{\widehat{p'}_i}\right\}_{i=0}^{M_{\text{eval}}-1}$ to the ground truth point cloud
	$\left\{p_i\right\}_{i=0}^{M_{\text{gt}}-1}$, for a point in the evalueated point cloud, the distance to the closest point in the ground truth
	point cloud is calculated. This is assumed to be the points ground truth position, since with several 100000-points in groundtruth, this point in 
	the ground truth point cloud should not be too far away from the orthogonal projection of the evaluated point. 
	
	\fig{img/orthProj.png}{Orthogonal projection of a point in the evaluated pointcloud ($\widehat{p_1}$) on the planar of the groundtruth point cloud.
	 The error $e$, determines how far the considerd point for the ground truth lies from the actual point of the ground truth.}{fig:orth}{0.7}
	
	This situation is displayed in fiture \ref{fig:orth}, where it gets clear, that the more points are available in the ground truth point cloud, 
	the smaller is the distance in between them and therefore the smaller the error $e$ becomes.

	Since calculating distances from several 100000 points to several 100000 points is computational very expensive, and in the current setup applying 
	it an all sequences and algorithms would require more than a day, only a subset of $P'$ of 1000 points per sequence and algorithms
	is taken into consideration. The indices for the subset $I_{\text{sub}}$ are sampled from a even distrubution of ${i}_{i = 0}^{M_{\text{eval}}-1}$. 
	Then, as mentioned, distances to the closest point in the ground truth point cloud is calculated for the sampled subset subset 
	$P'_{\text{sub}} = \left\{{\widehat{p'}_i}\right\}_{i \in I_{\text{sub}}} \subset P'$. 
	
	The error term for $\widehat{p}_i' \in P'_{\text{sub}}$ is then given by
	$$ e_i = \argmin_{j \in {i}_{i=0}^{M_{\text{gt}}-1}} \lVert \mathbf{\widehat{p}_i' - p_j} \rVert_2 $$.
	
	These error terms are then plotted within a boxplot for each method over all sequences. 
	
	\subsection{Density}
	
	As discribed in the second chapters, as a result of the functionality behind feature based methods, theire evaluated point clouds are 
	significantly less dense. To quantify the density, for each algorithm and sequence the absolut number of points generated by the 
	algorithm is accessed. 
	
\section{Computation Time}
	
	Since the computational performance of an algorithm is crutial to perform in real time, the absolute time that is needed to process each 
	sequence is measured for each algorithm. The time required for initialization is substracted, since it is not decisive for the assessment, 
	if the algorithm can be run in real time. For each sequence the resulting speed is additionally evaluated in computet frames per second. 
	
\section{Setup and Environment}

	\subsection{Evaluation}
	
	The entire evaluation is run on a virtual machine. The host system is a lenovo yoga with eight GB of RAM and the basic model (8250U CPU @1.6 
	GHz 1.80GHz) of an eight core i5. The operating system of the host machine is Windows 10 Home. The virtual
	machine is given 5 GB of Ram and 4 cores for the computations. The operating system of the virtual machine is Ubuntu 18.04. All further setup information can be extracted 
	from the github repository.

	\subsection{Flight Path Planning}